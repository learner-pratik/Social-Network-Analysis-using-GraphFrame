{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwqlp-6NcIop"
      },
      "outputs": [],
      "source": [
        "# installing spark in colab and creating spark session\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "from graphframes import GraphFrame\n",
        "from pyspark.sql.functions import count, desc, asc\n",
        " \n",
        "class Graphframes:\n",
        "    def __init__(self, inp):\n",
        "        \n",
        "        spark.sparkContext.addFile(inp)\n",
        "        self.reddit = spark.read.csv(\"file://\"+SparkFiles.get(\"soc-redditHyperlinks-body.tsv\"), sep=r'\\t', header = True)\n",
        "        \n",
        "        self.createGraph()\n",
        "        self.solutions()\n",
        "    def createGraph(self):\n",
        "        src = self.reddit.select(\"SOURCE_SUBREDDIT\").distinct()\n",
        "        dst = self.reddit.select(\"TARGET_SUBREDDIT\").distinct()\n",
        "        # spark dataframe union retains duplicates, hence we use distinct\n",
        "        self.vertices = src.union(dst).distinct().withColumnRenamed(\"SOURCE_SUBREDDIT\", \"id\")\n",
        "        \n",
        "        self.edges = self.reddit.select(\"SOURCE_SUBREDDIT\", \"TARGET_SUBREDDIT\", \"LINK_SENTIMENT\")\\\n",
        "        .withColumnRenamed(\"SOURCE_SUBREDDIT\", \"src\")\\\n",
        "        .withColumnRenamed(\"TARGET_SUBREDDIT\", \"dst\")\\\n",
        "        .withColumnRenamed(\"LINK_SENTIMENT\", \"sentiment\")\n",
        "        \n",
        "    def solutions(self):\n",
        "        g = GraphFrame(self.vertices, self.edges) # step that created the graph\n",
        "        g.cache()\n",
        "                \n",
        "        #Find the top 5 nodes with the highest outdegree and find the count of the number of outgoing edges in each\n",
        "        display(g.outDegrees.orderBy(desc(\"outDegree\")).head(5))\n",
        "        \n",
        "        #Find the top 5 nodes with the highest indegree and find the count of the number of incoming edges in each\n",
        "        display(g.inDegrees.orderBy(desc(\"inDegree\")).head(5))\n",
        "        \n",
        "        #Calculate PageRank for each of the nodes and output the top 5 nodes with the highest PageRank values\n",
        "        ranks = g.pageRank(maxIter=10)\n",
        "        ranks.cache()\n",
        "        display(ranks.vertices.orderBy(desc(\"pagerank\")).head(5))\n",
        "        \n",
        "        \n",
        "        !mkdir /content/checkpoints\n",
        "        # we need checkpointing to save temperory data used in big processes like finding connected components\n",
        "        sc.setCheckpointDir('/content/checkpoints')\n",
        "        \n",
        "        # Run the connected components algorithm on it and find the top 5 components with the largest number of nodes.\n",
        "        display(g.connectedComponents().orderBy(desc('component')).head(5))\n",
        "        \n",
        "        # Run the triangle counts algorithm on each of the vertices and output the top 5 vertices with the largest triangle count\n",
        "        display(g.triangleCount().orderBy(desc(\"count\")).head(5))\n",
        " \n",
        "Graphframes(\"https://snap.stanford.edu/data/soc-redditHyperlinks-body.tsv\")"
      ],
      "metadata": {
        "id": "f7UkukaicWGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}